\documentclass[11pt]{article}

\usepackage{morefloats}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools} % includes amsmath package
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{bm}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[us, 12hr]{datetime}
\usepackage{float}
\usepackage{bigstrut}
\usepackage{array}
\usepackage{tikz}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{gnuplottex}
\usepackage{gnuplot-lua-tikz}
\usepackage[backend=bibtex]{biblatex}

\addbibresource{bib.bib}

\lstset{language=Python, numbers=left, numberstyle=\footnotesize\color{UCDarkGray}, stepnumber=5, showspaces=false, columns=fixed, showstringspaces=false, breaklines=true, frame=single}

\widowpenalty=10000
\clubpenalty=10000

% ------------ Colors (Try to only use these so the theme is consistent)---------------------------
\definecolor{UCMaroon}{RGB}{128,0,0}
\definecolor{UCDarkGray}{RGB}{118,118,118}
\definecolor{UCLightGray}{RGB}{214,214,206}
\definecolor{UCRed}{RGB}{143,57,49}
\definecolor{UCYellowOrange}{RGB}{193,102,34}
\definecolor{UCLightGreen}{RGB}{138,157,69}
\definecolor{UCDarkGreen}{RGB}{88,89,63}
\definecolor{UCBlue}{RGB}{21,95,131}
\definecolor{UCViolet}{RGB}{53,14,32}

% ----------------------------- Header stuff ------------------------------------------------------
\pagestyle{fancy}
\lhead{Feingold, Riordan, and Whitaker}

% ---------------------------- Author/ title info --------------------------------------------------

\title{CMSC 23310 Final Project:\\
Distributed Hash Table}

\author{Josh Feingold \and Henry Riordan \and Jake Whitaker}

\date{Spring 2014}

% --------------------------------------------------------------------------------------------------

\begin{document}

\maketitle

\section{Project Overview}\label{sec:overview}

For this project we have decided on implementing a distributed hash table for our keystore. We focused on producing a key-value store that fits in the BASE model.\cite{Fox_1997_BASE} Our design of the distributed hash table came from Chord \cite{Stoica_2003_Chord}, Pastry \cite{Rowstron_2001_Pastry}, and Dynamo \cite{DeCandia_2007_Dynamo}. Overall our design followed that of Dynamo the closest as we liked some of the simple design choices. Our DHT like the others is based on a circular keyspace on which we place our nodes. These nodes will be in control of all keys between it and its predecessor. Many DHTs use virtual nodes to load balance and distribute replicas more evenly. Virtual nodes are not implemented in this design to reduce complexity.

The DHT's routing table is a complete routing table. This removed the complexity of the various routing methods discussed in \cite{Stoica_2003_Chord, Rowstron_2001_Pastry}. This follows the design of Dynamo \cite{DeCandia_2007_Dynamo}, it makes for a quicker response to requests due to the reduction to total messages required. Maintaining the routing table is slightly more complex as each node needs to keep track of all the other nodes and detect failures. Every node sends a heartbeat message to every other node at a regular interval, and the entries in the routing table contain the timestamp of the most recent heartbeat from each node. Every node regularly checks its routing table, and removes nodes from whom it hasn't received a heartbeat in an interval. Nodes can trigger replication and merging events by joining, dependent on their position in the keyspace.

(

All routing table entries have a timestamp that we compare to current time with a sweep function. Once the timeout period expires the node is removed from the routing table and is considered failed. To keep to all other nodes it sends a heartbeat message out to the entire network. When a node receives a heartbeat it resets the timeout for the sender in the routing table. If a node receives a heartbeat from a node not in its routing table it is considered a new node and it is added to our routing table. Nodes joining can also trigger replication and merging events depending on their position in the keyspace.

)

Our system uses replication for fault tolerance. Each node replicates the keyspaces of its two predecessors in its own keyspace. This allows for seamless rollover in the event of failure. In the event that a node fails, the rest of the nodes simply update their routing tables. A failure will trigger additional replication to ensure that there are two duplicates, but it will not reduce availability. There may be some latency if a request is made after a node fails, but before the routing tables are updated, but the system will always return a response.

(Our main fault tolerance is a set of two replicas. Each node replicates the keyspace of its two predecessors. The replica is stored in the same keystore as the nodes main data. This design allows for a node to fail and by simply having each node update its routing table the entire DHT is completely functional. A failure will trigger more replication to ensure two duplicates, but it will not reduce availability. There might be some additional latency if you make a request right after a node fails as the routing table update will take some time, however a response will be returned.)

Our worst case scenario is a partitioning of the network. If a node and its replicas are both on the same side of the partition, then nodes on the other side will be unable to retrieve them. The far side of the network will respond to get requests for those values with an error stating that the key could not be found. Our network remains available at the cost of consistency. When the network recovers from the partition, a simple merge function is run, and each key is set to the most recent value on either side of the partition. To facilitate this, the nodes store a timestamp with each key-value pair.

(The worst case in for our fault tolerance is when a network partition occurs. If a node and its two replicas are in one half of the network, the other half will not know those values anymore.  It will respond to get requests with an error stating that key is not set in the DHT. We will remain available with this method, just not consistent. When the network recovers from a partition a simple merge function is run and we choose the value that was set the latest using a timestamp that is stored with the key value pair.)

\section{Implementation}\label{sec:imple}

The DHT node is written in Python using the ZMQ python bindings. 

\subsection{Get}\label{sec:get}

When a node is sent a get message it first checks if it is the node that holds that key. If it holds that key, then it retrieves the value and returns a getResponse message. If it would hold that key, but the value is not set, an error message is returned. When a node receives a get message for a key it does not hold, it looks up the correct node using the complete routing table and then forwards the message to the correct node. The second node returns a value or error message to the first node who then responds to the client requesting the value.

\subsection{Set}\label{sec:set}

The set function is almost identical to that of the get except instead of just retrieving the value it sets the value and sends back a confirmation message to the client. Nodes forward messages in the same manner as in get when they receives messages that should go to different nodes.

\subsection{Heartbeats}\label{sec:HB}

Each node's routing table contains timestamps that are used to remove nodes when the entries are too old. In order to refresh these timestamps each node sends a heartbeat message to the entire network of nodes. These get set to the entire network, not just the nodes in the its routing table. This full network broadcast allows this system to use heartbeats to detect when a node rejoins the network. When a node receives a heartbeat message from a node it does not have in its routing table, it adds the node and then checks if it needs to adjust its replication and if it needs to merge part of its keystore with the new nodes keystore.

\subsection{Replication}\label{sec:Rep}

Our nodes replicate the data of their two predecessors. The replication of data achieved by having the owner of the key send the key, value, and timestamp to the its two successors in a replica message. This message can contain a single key or multiple keys as replica messages are sent in two distinct cases. First, whenever a node sets a value due to a set message being received it sends out a replica message with that key's data to the nodes replicating it. The second case is when a nodes keystore is changed due to a merge of if its replicas have changed due to nodes entering or leaving the network. In this case we send the entire keystore in a message to our replicas to ensure that they are completely up to date with our information.

\subsection{Merging}\label{sec:Merg}

When a node joins an existing network either from a partition than recovers or just recovering from a fail stop, a merge is required to integrate it into the network. When a node receives a heart beat message from a node not in its routing table, it checks if the new nodes successor is itself. If this is true then the old node must merge some of its keystore with the new node. The node giving up part of its keyspace sends a message containing all the key value pairs that exist in that space to the new node. The new node compares those values with the ones already in its space, if there is a conflict, the value with the most recent time stamp is the value that is kept. After the merge is completed, the new node sends replication messages.

\subsection{Deletion}\label{sec:Del}

A node cleans up its keystore when a new node enters the keyspace that it is replicating. This will happen after a mege with the node or a merge with the node predecessor. After this occurs the node then checks all keys in its keystore and removes those that are no longer owned by the nodes we are replicating.

\section{Example Scripts and Discussion}\label{sec:Ex}

\begin{lstlisting}[language={}, caption={Example Script 1}, basicstyle=\ttfamily]
start test
send {"destination": ["test"], "foo": "bar", "type": "foo"}
get test foo
send {"destination": ["test"], "bar": "baz", "type": "foo"}
set foo 42
get foo
\end{lstlisting}

\section{Conclusion}\label{sec:Conc}

\clearpage

\printbibliography

\end{document}

