\documentclass[11pt]{article}

\usepackage{morefloats}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools} % includes amsmath package
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{bm}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[us, 12hr]{datetime}
\usepackage{float}
\usepackage{bigstrut}
\usepackage{array}
\usepackage{tikz}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{gnuplottex}
\usepackage{gnuplot-lua-tikz}
\usepackage[backend=bibtex]{biblatex}

\addbibresource{bib.bib}

\lstset{language=Python, numbers=left, numberstyle=\footnotesize\color{UCDarkGray}, stepnumber=5, showspaces=false, columns=fixed, showstringspaces=false, breaklines=true, frame=single}

\widowpenalty=10000
\clubpenalty=10000

% ------------ Colors (Try to only use these so the theme is consistent)---------------------------
\definecolor{UCMaroon}{RGB}{128,0,0}
\definecolor{UCDarkGray}{RGB}{118,118,118}
\definecolor{UCLightGray}{RGB}{214,214,206}
\definecolor{UCRed}{RGB}{143,57,49}
\definecolor{UCYellowOrange}{RGB}{193,102,34}
\definecolor{UCLightGreen}{RGB}{138,157,69}
\definecolor{UCDarkGreen}{RGB}{88,89,63}
\definecolor{UCBlue}{RGB}{21,95,131}
\definecolor{UCViolet}{RGB}{53,14,32}

% ----------------------------- Header stuff ------------------------------------------------------
\pagestyle{fancy}
\lhead{Feingold, Riordan, and Whitaker}

% ---------------------------- Author/ title info --------------------------------------------------

\title{CMSC 23310 Final Project:\\
Distributed Hash Table}

\author{Josh Feingold \and Henry Riordan \and Jake Whitaker}

\date{Spring 2014}

% --------------------------------------------------------------------------------------------------

\begin{document}

\maketitle

\section{Project Overview}\label{sec:overview}

For this project we have decided on implementing a distributed hash table for our keystore. We focused on producing a key-value store that fits in the BASE model.\cite{Fox_1997_BASE} Our design of the distributed hash table came from Chord \cite{Stoica_2003_Chord}, Pastry \cite{Rowstron_2001_Pastry}, and Dynamo \cite{DeCandia_2007_Dynamo}. Overall our design followed that of Dynamo the closest as we liked some of the simple design choices. Our DHT like the others is based on a circular keyspace on which we place our nodes. These nodes will be in control of all keys between it and its predecessor. Many DHTs use virtual nodes to load balance and distribute replicas more evenly. Virtual nodes are not implemented in this design to reduce complexity.

The DHT's routing table is a complete routing table. This removed the complexity of the various routing methods discussed in \cite{Stoica_2003_Chord, Rowstron_2001_Pastry}. This follows the design of Dynamo \cite{DeCandia_2007_Dynamo}, it makes for a quicker response to requests due to the reduction to total messages required. Maintaining the routing table is slightly more complex as each node needs to keep track of all the other nodes and detect failures. Every node sends a heartbeat message to every other node at a regular interval, and the entries in the routing table contain the timestamp of the most recent heartbeat from each node. Every node regularly checks its routing table, and removes nodes from whom it hasn't received a heartbeat in an interval. Nodes can trigger replication and merging events by joining, dependent on their position in the keyspace.

(

All routing table entries have a timestamp that we compare to current time with a sweep function. Once the timeout period expires the node is removed from the routing table and is considered failed. To keep to all other nodes it sends a heartbeat message out to the entire network. When a node receives a heartbeat it resets the timeout for the sender in the routing table. If a node receives a heartbeat from a node not in its routing table it is considered a new node and it is added to our routing table. Nodes joining can also trigger replication and merging events depending on their position in the keyspace.

)

Our system uses replication for fault tolerance. Each node replicates the keyspaces of its two predecessors in its own keyspace. This allows for seamless rollover in the event of failure. In the event that a node fails, the rest of the nodes simply update their routing tables. A failure will trigger additional replication to ensure that there are two duplicates, but it will not reduce availability. There may be some latency if a request is made after a node fails, but before the routing tables are updated, but the system will always return a response.

(Our main fault tolerance is a set of two replicas. Each node replicates the keyspace of its two predecessors. The replica is stored in the same keystore as the nodes main data. This design allows for a node to fail and by simply having each node update its routing table the entire DHT is completely functional. A failure will trigger more replication to ensure two duplicates, but it will not reduce availability. There might be some additional latency if you make a request right after a node fails as the routing table update will take some time, however a response will be returned.)

Our worst case scenario is a partitioning of the network. If a node and its replicas are both on the same side of the partition, then nodes on the other side will be unable to retrieve them. The far side of the network will respond to get requests for those values with an error stating that the key could not be found. Our network remains available at the cost of consistency. When the network recovers from the partition, a simple merge function is run, and each key is set to the most recent value on either side of the partition. To facilitate this, the nodes store a timestamp with each key-value pair.

(The worst case in for our fault tolerance is when a network partition occurs. If a node and its two replicas are in one half of the network, the other half will not know those values anymore.  It will respond to get requests with an error stating that key is not set in the DHT. We will remain available with this method, just not consistent. When the network recovers from a partition a simple merge function is run and we choose the value that was set the latest using a timestamp that is stored with the key value pair.)

\section{Implementation}\label{sec:imple}

The DHT node is written in Python using the ZMQ python bindings. 

\subsection{Set}\label{sec:get}

When a node receives a set message from the broker, two possibilities exist: Either it is the owner of the key, or it is not. If it is the owner of the key, it adds or updates that value in its keyspace, and returns a set response to the broker, and sends replica messages to the next two nodes on the ring.

If it does not own the key, it forwards the get message to the correct node, adding a field that indicates itself as the source of the message. Additionally, to deal with the possibility of network partitions or fail-stops, the message is stored in a dictionary, with the key being the message's ID field. If the destination node is removed from the routing table before the sender receives a response, the message is resent to a new destination. When a response is received, the sender removes the message from the queue and sends a set response message back to the broker.

The destination node uses a similar method. The only difference are that the node sends a 'set relay' message to the source node instead of a response to the broker. Furthermore, in the rare event that the message must be forwarded a second time, the node does not update the source field or add it to the pending messages queue. This prevents duplicate responses.

\subsection{Get}\label{sec:get}

Get requests are processed almost identically to set requests, with the difference being that the node retrieves the value and returns it to the client, rather than updating it. Get messages are forwarded and stored in the same manner as set messages. The only difference is that, while a set request will always be successful, a get request may return an error in the event that the node cannot communicate with the owner of a key, or any of its replicas.

\subsection{Heartbeats}\label{sec:HB}

Each node's routing table contains timestamps indicating the time of the last heartbeat from each node. Any nodes whose heartbeats were older than a chosen age are removed from the routing table. To refresh these timestamps, each node sends a heartbeat to the entire network on regular intervals. These heartbeat messages get sent to every node in the network, rather than every node in the routing table. This full network broadcast allows for the "rediscovery" of nodes in the aftermath of a network partition or a failure. When a node A receives a heartbeat from a node B that is not in its routing table, B is added. If B is A's immediate predecessor, then A may send a merge message to B. If B is A's immediate, or secondary successor, then A must send replica messages to B.

\subsection{Replication}\label{sec:Rep}

Each node replicates the data of its two predecessors. This is achieved by having the owner of the key send the key, value, and timestamp of each item in its keyspace in a replica message. This message can contain a single key, or multiple keys depending on the circumstance in which the message was sent. Firstly, whenever a node sets a value in response to a set message, it sends out a replica message. Secondly, when a node's keystore is changed in response to a merge, it must send a replica of its entire keystore to ensure consistency between replicas.

\subsection{Merging}\label{sec:Merg}

When a node joins an existing network, either from a recovering partition, or just recovering from a fail-stop, a merge is required to integrate it into the network. When a node receives a heartbeat from a node not in its routing table, it checks if the new node is its immediate predecessor. If so, then the node checks if any of the items in its keystore should now belong to the new node, and sends any such items to the new node in a merge message. The new node resolves any conflicts by choosing the most recently set value, and then sends replication messages for the final values.

\subsection{Deletion}\label{sec:Del}

A node cleans up its keystore when a new node enters the keyspace that it is replicating. This will happen after a mege with the node or a merge with the node predecessor. After this occurs the node then checks all keys in its keystore and removes those that are no longer owned by the nodes we are replicating.

\section{Example Scripts and Discussion}\label{sec:Ex}


\section{Conclusion}\label{sec:Conc}

\clearpage

\printbibliography

\clearpage

\appendix

\section{Test Scripts Source}

\begin{lstlisting}[language={}, caption={Fail Stop Test Script}, basicstyle=\ttfamily]
start Alpha --peer-names Beta,Gamma,Omega,Able,Baker,Charlie,Delta 
start Beta --peer-names Alpha,Gamma,Omega,Able,Baker,Charlie,Delta 
start Gamma --peer-names Alpha,Beta,Omega,Able,Baker,Charlie,Delta 
start Omega --peer-names Alpha,Beta,Gamma,Able,Baker,Charlie,Delta 
start Able --peer-names Alpha,Beta,Gamma,Omega,Baker,Charlie,Delta 
start Baker --peer-names Alpha,Beta,Gamma,Omega,Able,Charlie,Delta 
start Charlie --peer-names Alpha,Beta,Gamma,Omega,Able,Baker,Delta 
start Delta --peer-names Alpha,Beta,Gamma,Omega,Able,Baker,Charlie 
set A 0
set B 1
set C 2
set D 3
set E 4
set F 5
set G 6
set H 7
set I 8
set J 9
set K 10
set L 11
set M 12
set N 13
set O 14
set P 15
set Q 16
set R 17
set S 18
set T 19
set U 20
set V 21
set W 22
set X 23
set Y 24
set Z 25
set a 26
set b 27
set c 28
set d 29
set e 30
set f 31
set g 32
set h 33
set i 34
set j 35
set k 36
set l 37
set m 38
set n 39
set o 40
set p 41
set q 42
set r 43
set s 44
set t 45
set u 46
set v 47
set w 48
set x 49
set y 50
set z 51
after 300{
split part1 Able,Baker,Charlie,Delta
}
after 1000{
get Able A
get Baker B
get Charlie C
get Delta D
get Able E
get Baker F
get Charlie G
get Delta H
get Able I
get Baker J
get Charlie K
get Delta L
get Able M
get Baker N
get Charlie O
get Delta P
get Able Q
get Baker R
get Charlie S
get Delta T
get Able U
get Baker V
get Charlie W
get Delta X
get Able Y
get Baker Z
get Charlie a
get Delta b
get Able c
get Baker d
get Charlie e
get Delta f
get Able g
get Baker h
get Charlie i
get Delta j
get Able k
get Baker l
get Charlie m
get Delta n
get Able o
get Baker p
get Charlie q
get Delta r
get Able s
get Baker t
get Charlie u
get Delta v
get Able w
get Baker x
get Charlie y
get Delta z
}
\end{lstlisting}


\begin{lstlisting}[language={}, caption={Partition Merge Test Script}, basicstyle=\ttfamily]
start Alpha --peer-names Beta,Gamma,Omega,Able,Baker,Charlie,Delta 
start Beta --peer-names Alpha,Gamma,Omega,Able,Baker,Charlie,Delta 
start Gamma --peer-names Alpha,Beta,Omega,Able,Baker,Charlie,Delta 
start Omega --peer-names Alpha,Beta,Gamma,Able,Baker,Charlie,Delta 
start Able --peer-names Alpha,Beta,Gamma,Omega,Baker,Charlie,Delta 
start Baker --peer-names Alpha,Beta,Gamma,Omega,Able,Charlie,Delta 
start Charlie --peer-names Alpha,Beta,Gamma,Omega,Able,Baker,Delta 
start Delta --peer-names Alpha,Beta,Gamma,Omega,Able,Baker,Charlie  
split part1 Alpha,Beta,Gamma,Delta
#split part2 Able,Baker,Charlie,Omega # set the values on one half of the network
set Alpha a 11
set Baker b 22
set Gamma c 13
set Omega d 14
set Alpha e 15
set Beta f 16
set Gamma g 17
set Omega h 18
set Able a 21
set Beta b 12
set Charlie c 23
set Delta d 24
set Able e 25
set Baker f 26
set Charlie g 27
set Delta h 28
join part1
after 200 {
get a
get b
get c
get d
get e
get f
get g
get h
}
\end{lstlisting}

\end{document}

