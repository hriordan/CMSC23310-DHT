\documentclass[11pt]{article}

\usepackage{morefloats}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools} % includes amsmath package
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{bm}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[us, 12hr]{datetime}
\usepackage{float}
\usepackage{bigstrut}
\usepackage{array}
\usepackage{tikz}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{gnuplottex}
\usepackage{gnuplot-lua-tikz}
\usepackage[backend=bibtex]{biblatex}

\addbibresource{bib.bib}

\lstset{language=Python, numbers=left, numberstyle=\footnotesize\color{UCDarkGray}, stepnumber=5, showspaces=false, columns=fixed, showstringspaces=false, breaklines=true, frame=single}

\widowpenalty=10000
\clubpenalty=10000

% ------------ Colors (Try to only use these so the theme is consistent)---------------------------
\definecolor{UCMaroon}{RGB}{128,0,0}
\definecolor{UCDarkGray}{RGB}{118,118,118}
\definecolor{UCLightGray}{RGB}{214,214,206}
\definecolor{UCRed}{RGB}{143,57,49}
\definecolor{UCYellowOrange}{RGB}{193,102,34}
\definecolor{UCLightGreen}{RGB}{138,157,69}
\definecolor{UCDarkGreen}{RGB}{88,89,63}
\definecolor{UCBlue}{RGB}{21,95,131}
\definecolor{UCViolet}{RGB}{53,14,32}

% ----------------------------- Header stuff ------------------------------------------------------
\pagestyle{fancy}
\lhead{Feingold, Riordan, and Whitaker}

% ---------------------------- Author/ title info --------------------------------------------------

\title{CMSC 23310 Final Project:\\
Distributed Hash Table}

\author{Josh Feingold \and Henry Riordan \and Jake Whitaker}

\date{Spring 2014}

% --------------------------------------------------------------------------------------------------

\begin{document}

\maketitle

\section{Project Overview}\label{sec:overview}

For this project we have decided on implementing a distributed hash table for our keystore. We focused on producing a key-value store that fits in the BASE model.\cite{Fox_1997_BASE} Our design of the distributed hash table came from Chord \cite{Stoica_2003_Chord}, Pastry \cite{Rowstron_2001_Pastry}, and Dynamo \cite{DeCandia_2007_Dynamo}. Overall our design followed that of Dynamo the closest as we liked some of the simple design choices. Our DHT like the others is based on a circular keyspace on which we place our nodes. These nodes will be in control of all keys between it and its predecessor. 

The DHT's routing table is a complete routing table. This removed the complexity of the various routing methods discussed in \cite{Stoica_2003_Chord, Rowstron_2001_Pastry}. This follows the design of Dynamo \cite{DeCandia_2007_Dynamo}, it makes for a quicker response to requests due to the reduction to total messages required. Maintaining the routing table is slightly more complex as each node needs to keep track of all the other nodes and detect failures. All routing table entries have a timestamp that we compare to current time with a sweep function. Once the timeout period expires the node is removed from the routing table and is considered failed.  In order for a node to prove to all other nodes it sends a heartbeat message out to the entire network. When a node receives a heartbeat it resets the timeout for the sender in the routing table. If a node receives a heartbeat from a node not in its routing table it is considered a new node and it is added to our routing table. Nodes joining can also trigger replication and merging events depending on their position in the keyspace.

Our main fault tolerance is a set of two replicas. Each node replicates the keyspace of its two predecessors. This design allows for a node to fail and by simply having each node update its routing table the entire DHT is completely functional. A failure will trigger more replication to ensure two duplicates, but it will not reduce availability. There might be some additional latency if you make a request right after a node fails as the routing table update will take some time, however a response will be returned.

The worst case in for our fault tolerance is when a network partition occurs. If a node and its two replicas are in one half of the network, the other half will not know those values anymore.  It will respond to get requests with an error stating that key is not set in the DHT. We will remain available with this method, just not consistent. When the network recovers from a partition a simple merge function is run and we choose the value that was set the latest using a timestamp that is stored with the key value pair.

%%%%% No virtual nodes

\section{Implementation}\label{sec:imple}

Our implementation follows closely to that of Dynamo. We have a complete routing table and 2 replicas of the key-value pairs.

\subsection{Get}\label{sec:get}

Our get function is very simple. When a get request comes into a node the following occurs. The node checks to see if the key belongs to itself. If it does it will retrieve the key and send the get response message to the broker. If it does not own the key, the node will forward the get request to the correct node, using the complete routing table. To ensure that the get message was received by the second node, that node will send the get response back to the first node who then responds to the broker.

\subsection{Set}\label{sec:set}

The set function first checks if the key being set is in our section of the keyspace. If not we forward the message to the correct node, who after completing this function verifies completion by sending a message to the original node. If the key is in our section of the keyspace we set the value in our keystore and then send two replication messages, one to our successor and one to our successor's successor. This creates 2 replicas in case the first node fails. After completion we either send a setResponse to the broker or the node who forwarded the request.

\subsection{Heartbeats}\label{sec:HB}

Every \textbf{[HEART BEAT TIME]} we send a heartbeat message to all nodes on the network. We broadcast this even beyond our routing table in case a new node enters the network that we need to announce ourselves to. These heartbeats are used to track what nodes on the network are still alive and their absence is how we detect failed nodes and partitions.

\subsection{Replication}\label{sec:Rep}

A node only replicates keys it is sent on a replicate message. It stores these keys in the same keystore as its main keys so that if its predecessor fails, it will automatically be able to handle the requests for that keyspace. A node will only delete keys from its keystore if it sees a new node enter that reduces the keyspace that this node would replicate. This check occurs when the routing table is changed due to a heartbeat from an unknown node.

\subsection{Merging}\label{sec:Merg}



\section{Example Scripts and Discussion}\label{sec:Ex}

\begin{lstlisting}[language={}, caption={Example Script 1}, basicstyle=\ttfamily]
start test
send {"destination": ["test"], "foo": "bar", "type": "foo"}
get test foo
send {"destination": ["test"], "bar": "baz", "type": "foo"}
set foo 42
get foo
\end{lstlisting}

\section{Conclusion}\label{sec:Conc}

\clearpage

\printbibliography

\end{document}

